{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ee9193",
   "metadata": {},
   "source": [
    "# Data preparation for StorSeismic fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04347607",
   "metadata": {},
   "source": [
    "This notebook contains data preparation for fine-tuning of StorSeismic (Harsuko and Alkhalifah, 2022). Here, we will prepare a dataset for denoising and velocity estimation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0352b",
   "metadata": {},
   "source": [
    "## Load required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b378b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from snist.dataset import SNIST\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import storseismic.utils as utils\n",
    "\n",
    "params = {\n",
    "    'savefig.dpi': 600,  # to adjust notebook inline plot size\n",
    "    'figure.dpi' : 600,\n",
    "    'axes.labelsize':14,  # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize':14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'legend.fontsize': 14,  # was 10\n",
    "    'xtick.labelsize':12,\n",
    "    'ytick.labelsize':12,\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5ca4a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9de425",
   "metadata": {},
   "source": [
    "### Load/download data from SNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ba80d",
   "metadata": {},
   "source": [
    "https://github.com/LukasMosser/SNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2404d504",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "snist_train = SNIST('./', train=True, download=True) # Training data\n",
    "snist_test = SNIST('./', train=False, download=True, noise=0) # Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed48f54",
   "metadata": {},
   "source": [
    "### Create data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1f938",
   "metadata": {},
   "source": [
    "We create four instances in the dictionary:\n",
    "- <code>inputs_embeds</code> for the noisy data\n",
    "- <code>labels</code> for the clean data\n",
    "- <code>vel</code> for the velocities\n",
    "- <code>index</code> to store the original, unaugmented indices of the data\n",
    "- <code>noise_lvl</code> to store the noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03207bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snist_train_mlm, snist_test_mlm = {}, {}\n",
    "for data, data_mlm in zip([snist_train, snist_test], \n",
    "                [snist_train_mlm, snist_test_mlm]):\n",
    "    data_mlm['inputs_embeds'] = data.train_data.detach().clone()\n",
    "    data_mlm['labels'] =  data.train_data.detach().clone()\n",
    "    data_mlm['vel'] = data.targets.detach().clone()\n",
    "    data_mlm['index'] = torch.arange(data.train_data.shape[0])\n",
    "    data_mlm['noise_lvl'] = torch.zeros(data.train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04763ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some cleanup (swap time and offset axis, and remove extra axis)\n",
    "\n",
    "for data in snist_train_mlm, snist_test_mlm:\n",
    "    for key in data.keys():\n",
    "        if key not in ['index', 'vel', 'noise_lvl']:\n",
    "            data[key] = data[key].swapaxes(2, 3)\n",
    "            data[key] = data[key].squeeze()\n",
    "            \n",
    "print(\"Training data shape: \", snist_train_mlm['inputs_embeds'].shape)\n",
    "print(\"Testing data shape: \", snist_test_mlm['inputs_embeds'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183dcae4",
   "metadata": {},
   "source": [
    "## Pre-processing and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e02d0",
   "metadata": {},
   "source": [
    "### Scale the data into the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71362608",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin_all = torch.min(snist_train_mlm['inputs_embeds'])\n",
    "vmax_all = torch.max(snist_train_mlm['inputs_embeds'])\n",
    "\n",
    "for data_mlm in [snist_train_mlm, snist_test_mlm]:\n",
    "    data_mlm['inputs_embeds'] = -1 + (2 * (data_mlm['inputs_embeds'] - vmin_all) / (vmax_all - vmin_all))\n",
    "    data_mlm['labels'] = -1 + (2 * (data_mlm['labels'] - vmin_all) / (vmax_all - vmin_all))\n",
    "    \n",
    "vmin_all = -1\n",
    "vmax_all = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab195df4",
   "metadata": {},
   "source": [
    "### Add noise to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "162cfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(input_data, sigma=1):\n",
    "    output_data = input_data.detach().clone()\n",
    "    output_data = output_data + (torch.randn(input_data.size()) * sigma * torch.sqrt(torch.var(input_data)))\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "# Add 1-sigma Gaussian noise to 40% data\n",
    "snist_train_mlm['inputs_embeds'][120:360] = add_noise(snist_train_mlm['inputs_embeds'][120:360], sigma=1)\n",
    "snist_train_mlm['noise_lvl'][120:360] = 1\n",
    "snist_test_mlm['inputs_embeds'][30:90] = add_noise(snist_test_mlm['inputs_embeds'][30:90], sigma=1)\n",
    "snist_test_mlm['noise_lvl'][30:90] = 1\n",
    "\n",
    "# Add 2-sigma Gaussian noise to 40% data\n",
    "snist_train_mlm['inputs_embeds'][360:] = add_noise(snist_train_mlm['inputs_embeds'][360:], sigma=2)\n",
    "snist_train_mlm['noise_lvl'][360:] = 2\n",
    "snist_test_mlm['inputs_embeds'][90:] = add_noise(snist_test_mlm['inputs_embeds'][90:], sigma=2)\n",
    "snist_test_mlm['noise_lvl'][90:] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010204e8",
   "metadata": {},
   "source": [
    "### Apply polarity reversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb3920",
   "metadata": {},
   "source": [
    "Polarity reversal augmentation: We reverse the polarity of the data, as if we use a reversed polarity of the source. This is done simply by multiplying the data by -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b1c318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_change = True\n",
    "\n",
    "if polarity_change:\n",
    "    for data in snist_train_mlm, snist_test_mlm:\n",
    "        augmented = data.copy()\n",
    "        augmented['inputs_embeds'] = augmented['inputs_embeds'] * -1\n",
    "        augmented['labels'] = augmented['labels'] * -1\n",
    "        for key in augmented.keys():\n",
    "            data[key] = torch.cat((data[key], augmented[key]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb49c0",
   "metadata": {},
   "source": [
    "## Finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55a260",
   "metadata": {},
   "source": [
    "### Wrap in a custom Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "187cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.SSDataset(snist_train_mlm)\n",
    "test_data = utils.SSDataset(snist_test_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89514400",
   "metadata": {},
   "source": [
    "### Visualize created dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0da989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "idx =  torch.randint(train_data.encodings['inputs_embeds'].shape[0], (4,))\n",
    "\n",
    "for inputs_embeds, labels, noise_lvl, vel in zip(train_data.encodings['inputs_embeds'][idx], \n",
    "                                                 train_data.encodings['labels'][idx], \n",
    "                                                 train_data.encodings['noise_lvl'][idx],\n",
    "                                                 train_data.encodings['vel'][idx]):\n",
    "    f, ax = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    ax[0].imshow(inputs_embeds.swapaxes(0, 1) - inputs_embeds.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[0].set_title(\"Input (Noise level: {})\".format(int(noise_lvl)))\n",
    "    ax[1].imshow(labels.swapaxes(0, 1) - labels.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[1].set_title(\"Label\")\n",
    "    ax[2].step(np.pad(vel, (0, 1), 'edge'), np.arange(vel.shape[-1] + 1))\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].set_title(\"Velocity\")\n",
    "    ax[2].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "\n",
    "idx =  torch.randint(test_data.encodings['inputs_embeds'].shape[0], (4,))\n",
    "\n",
    "for inputs_embeds, labels, noise_lvl, vel in zip(test_data.encodings['inputs_embeds'][idx], \n",
    "                                                 test_data.encodings['labels'][idx], \n",
    "                                                 test_data.encodings['noise_lvl'][idx],\n",
    "                                                 test_data.encodings['vel'][idx]):\n",
    "    f, ax = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    ax[0].imshow(inputs_embeds.swapaxes(0, 1) - inputs_embeds.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[0].set_title(\"Input (Noise level: {})\".format(int(noise_lvl)))\n",
    "    ax[1].imshow(labels.swapaxes(0, 1) - labels.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[1].set_title(\"Label\")\n",
    "    ax[2].step(np.pad(vel, (0, 1), 'edge'), np.arange(vel.shape[-1] + 1))\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].set_title(\"Velocity\")\n",
    "    ax[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce478f",
   "metadata": {},
   "source": [
    "### Re-check dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ab1f0",
   "metadata": {},
   "source": [
    "We applied x 2 (polarity reversal) times of augmentation to each sample. Therefore, the total number of each of training and testing dataset must have been expanded 2 times than the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape: \", train_data.encodings['inputs_embeds'].shape)\n",
    "print(\"Testing data shape: \", test_data.encodings['inputs_embeds'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f6983",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"./data/finetune\"\n",
    "\n",
    "if not os.path.exists(parent_dir):\n",
    "    os.makedirs(parent_dir)\n",
    "    torch.save(train_data, os.path.join(parent_dir, \"train_data.pt\"))\n",
    "    torch.save(test_data, os.path.join(parent_dir, \"test_data.pt\"))\n",
    "    print(\"Saved successfully\")\n",
    "elif os.path.exists(parent_dir):\n",
    "    check = input(\"Directory exist, overwrite? (y/n)\")\n",
    "    if check == 'y':\n",
    "        torch.save(train_data, os.path.join(parent_dir, \"train_data.pt\"))\n",
    "        torch.save(test_data, os.path.join(parent_dir, \"test_data.pt\"))\n",
    "        print(\"Saved successfully\")\n",
    "    else:\n",
    "        print(\"Saving failed\")\n",
    "else:\n",
    "    print(\"Saving failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147efbf8",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f5c52",
   "metadata": {},
   "source": [
    "Harsuko, R., and T. Alkhalifah, 2022, Storseismic: An approach to pre-train a neural network to store seismic data features: 83rd EAGE Annual Conference & Exhibition, European Association of Geoscientists & Engineers, 1â€“5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storseismic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
