{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7abb33",
   "metadata": {},
   "source": [
    "# Data preparation for StorSeismic pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd58060",
   "metadata": {},
   "source": [
    "This notebook contains data preparation for pre-training of StorSeismic (Harsuko and Alkhalifah, 2022). The goal here is to produce a dataset containing input seismic data which are masked trace-wise with their corresponding unmasked data as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad7abc",
   "metadata": {},
   "source": [
    "## Load required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b378b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from snist.dataset import SNIST\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import storseismic.utils as utils\n",
    "\n",
    "params = {\n",
    "    'savefig.dpi': 600,  # to adjust notebook inline plot size\n",
    "    'figure.dpi' : 600,\n",
    "    'axes.labelsize':14,  # fontsize for x and y labels (was 10)\n",
    "    'axes.titlesize':14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'legend.fontsize': 14,  # was 10\n",
    "    'xtick.labelsize':12,\n",
    "    'ytick.labelsize':12,\n",
    "    'font.family': 'serif',\n",
    "}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d80ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16984f49",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c2bc8",
   "metadata": {},
   "source": [
    "### Load/download data from SNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a22417",
   "metadata": {},
   "source": [
    "https://github.com/LukasMosser/SNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2404d504",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "snist_train = SNIST('./', train=True, download=True) # Training data\n",
    "snist_test = SNIST('./', train=False, download=True, noise=0) # Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897bc00",
   "metadata": {},
   "source": [
    "### Create data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d0236",
   "metadata": {},
   "source": [
    "We create four instances in the dictionary:\n",
    "- <code>inputs_embeds</code> for the input\n",
    "- <code>labels</code> for the label\n",
    "- <code>mask_label</code> to mark the masked indices (will useful later in in the pre-training)\n",
    "- <code>index</code> to store the original, unaugmented indices of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03207bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snist_train_mlm, snist_test_mlm = {}, {}\n",
    "for data, data_mlm in zip([snist_train, snist_test], \n",
    "                [snist_train_mlm, snist_test_mlm]):\n",
    "    data_mlm['inputs_embeds'] = data.train_data.detach().clone()\n",
    "    data_mlm['labels'] =  data.train_data.detach().clone()\n",
    "    data_mlm['mask_label'] = torch.zeros_like(data.train_data)\n",
    "    data_mlm['index'] = torch.arange(data.train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04763ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some cleanup (swap time and offset axis, and remove extra axis)\n",
    "\n",
    "for data in snist_train_mlm, snist_test_mlm:\n",
    "    for key in data.keys():\n",
    "        if key != 'index':\n",
    "            data[key] = data[key].swapaxes(2, 3)\n",
    "            data[key] = data[key].squeeze()\n",
    "            \n",
    "print(\"Training data shape: \", snist_train_mlm['inputs_embeds'].shape)\n",
    "print(\"Testing data shape: \", snist_test_mlm['inputs_embeds'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c7a19",
   "metadata": {},
   "source": [
    "## Pre-processing and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5a050",
   "metadata": {},
   "source": [
    "### Scale the data into the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71362608",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin_all = torch.min(snist_train_mlm['inputs_embeds'])\n",
    "vmax_all = torch.max(snist_train_mlm['inputs_embeds'])\n",
    "\n",
    "for data_mlm in [snist_train_mlm, snist_test_mlm]:\n",
    "    data_mlm['inputs_embeds'] = -1 + (2 * (data_mlm['inputs_embeds'] - vmin_all) / (vmax_all - vmin_all))\n",
    "    data_mlm['labels'] = -1 + (2 * (data_mlm['labels'] - vmin_all) / (vmax_all - vmin_all))\n",
    "    \n",
    "vmin_all = -1\n",
    "vmax_all = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45598d",
   "metadata": {},
   "source": [
    "### Multiply the data N-times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7267e5",
   "metadata": {},
   "source": [
    "So that all the masking will be applied N-times (differently) to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1325ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_factor = 10\n",
    "\n",
    "for key in snist_train_mlm.keys():\n",
    "    snist_train_mlm[key] = snist_train_mlm[key].repeat(mult_factor, 1, 1)\n",
    "snist_train_mlm['index'] = torch.arange(snist_train_mlm['inputs_embeds'].shape[0])\n",
    "    \n",
    "for key in snist_test_mlm.keys():\n",
    "    snist_test_mlm[key] = snist_test_mlm[key].repeat(mult_factor, 1, 1)\n",
    "snist_test_mlm['index'] = torch.arange(snist_test_mlm['inputs_embeds'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fe95d",
   "metadata": {},
   "source": [
    "### Apply shifts in time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6748f",
   "metadata": {},
   "source": [
    "Time shifting augmentation: We shift each shot gather upward/downward in time randomly within the defined shifting range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca479ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shift = 2 # Number of shifts performed for each sample (+ we keep the original, unshifted data)\n",
    "min_shift_mag = -25 # Max shift upward in time sample\n",
    "max_shift_mag = 25 # Max shift downward in time sample\n",
    "\n",
    "filler = torch.mean(snist_train_mlm['inputs_embeds'])\n",
    "for data in snist_train_mlm, snist_test_mlm:\n",
    "    data_len = data['inputs_embeds'].shape[0]\n",
    "    for n in range(n_shift):\n",
    "        data2 = {key: value[:data_len] for key, value in data.items()}\n",
    "        for i in range(data_len):\n",
    "            while True:\n",
    "                shift_mag = int(torch.randint(low=min_shift_mag-1, high=max_shift_mag+1, size=(1, )))\n",
    "                if shift_mag != 0:\n",
    "                    break\n",
    "            data2['inputs_embeds'][i] = torch.roll(data2['inputs_embeds'][i], shift_mag, -1)\n",
    "            data2['labels'][i] = torch.roll(data2['labels'][i], shift_mag, -1)\n",
    "            if shift_mag > 0:\n",
    "                data2['inputs_embeds'][i, :, :shift_mag] = filler\n",
    "                data2['labels'][i, :, :shift_mag] = filler\n",
    "            elif shift_mag < 0:\n",
    "                data2['inputs_embeds'][i, :, data2['inputs_embeds'].shape[-1]+shift_mag:] = filler\n",
    "                data2['labels'][i, :, data2['inputs_embeds'].shape[-1]+shift_mag:] = filler\n",
    "\n",
    "        for key in data.keys():\n",
    "            data[key] = torch.cat((data[key], data2[key]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2817716",
   "metadata": {},
   "source": [
    "### Mask the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6d9eb",
   "metadata": {},
   "source": [
    "Here we mask the shot gather trace-wise. Following Devlin et al. (2018), we use a mask proportion of 15% for each shot gather (3 out of 20 traces) with the details of the following:\n",
    "- 80% of the time we mask with a [mask] token (i.e., random numbers drawn from Gaussian distribution)\n",
    "- 10% of the time we replace with another trace from the same shot gather\n",
    "- 10% of the time we keep the trace as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab078ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "mask_token = torch.randn(1, 1, snist_train_mlm['inputs_embeds'].shape[-1]) # The [mask] token\n",
    "mask_token = -1 + (2 * (mask_token - torch.min(mask_token)) / (torch.max(mask_token) - torch.min(mask_token))) # Scale the [mask] token to [-1, 1]\n",
    "\n",
    "def mask_all(data, mask_proportion=.15):\n",
    "    seq_len = data['inputs_embeds'].shape[1]\n",
    "    for i in range(data['inputs_embeds'].shape[0]):\n",
    "        muted_idx = torch.randperm(seq_len)[:int(np.floor(seq_len * mask_proportion))]\n",
    "        for j in muted_idx:\n",
    "            prob = torch.rand(1)\n",
    "            if prob < 0.8:\n",
    "                data['inputs_embeds'][i, j, :] = mask_token\n",
    "            elif prob >= 0.8 and prob < 0.9:\n",
    "                switch_idx = torch.where(torch.arange(seq_len) != j)[0][torch.randint(high=seq_len-1, size=(1,))]\n",
    "                data['inputs_embeds'][i, j, :] = data['inputs_embeds'][i, switch_idx, :].squeeze(-1)\n",
    "            else:\n",
    "                data['inputs_embeds'][i, j, :] = data['inputs_embeds'][i, j, :]\n",
    "\n",
    "        data['mask_label'][i, muted_idx] = 1\n",
    "            \n",
    "    return data\n",
    "\n",
    "for data_mlm in snist_train_mlm, snist_test_mlm:\n",
    "    data_mlm = mask_all(data_mlm, mask_proportion=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689adb7",
   "metadata": {},
   "source": [
    "### Apply polarity reversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47615e5b",
   "metadata": {},
   "source": [
    "Polarity reversal augmentation: We reverse the polarity of the data, as if we use a reversed polarity of the source. This is done simply by multiplying the data by -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1c318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_change = True\n",
    "\n",
    "if polarity_change:\n",
    "    for data in snist_train_mlm, snist_test_mlm:\n",
    "        augmented = data.copy()\n",
    "        augmented['inputs_embeds'] = augmented['inputs_embeds'] * -1\n",
    "        augmented['labels'] = augmented['labels'] * -1\n",
    "        for key in augmented.keys():\n",
    "            data[key] = torch.cat((data[key], augmented[key]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ecbbbd",
   "metadata": {},
   "source": [
    "## Finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2478e",
   "metadata": {},
   "source": [
    "### Wrap in a custom Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.SSDataset(snist_train_mlm)\n",
    "test_data = utils.SSDataset(snist_test_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4dea2d",
   "metadata": {},
   "source": [
    "### Visualize created dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0da989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "idx =  torch.randint(train_data.encodings['inputs_embeds'].shape[0], (4,))\n",
    "\n",
    "for inputs_embeds, labels in zip(train_data.encodings['inputs_embeds'][idx], \n",
    "                                 train_data.encodings['labels'][idx]):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(inputs_embeds.swapaxes(0, 1) - inputs_embeds.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(labels.swapaxes(0, 1) - labels.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[1].set_title(\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "\n",
    "idx = torch.randint(test_data.encodings['inputs_embeds'].shape[0], (4,))\n",
    "\n",
    "for inputs_embeds, labels, mask in zip(test_data.encodings['inputs_embeds'][idx], \n",
    "                                 test_data.encodings['labels'][idx],\n",
    "                                test_data.encodings['mask_label'][idx]):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(inputs_embeds.swapaxes(0, 1) - inputs_embeds.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(labels.swapaxes(0, 1) - labels.mean(), \n",
    "                 aspect=.1, vmin=vmin_all, vmax=vmax_all, cmap='seismic', interpolation='none')\n",
    "    ax[1].set_title(\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd8bf8",
   "metadata": {},
   "source": [
    "### Re-check dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3e4b6",
   "metadata": {},
   "source": [
    "We applied 10 (masking) x 3 (time shift) x 2 (polarity reversal) times of augmentation to each sample. Therefore, the total number of each of training and testing dataset must have been expanded 60 times than the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data shape: \", train_data.encodings['inputs_embeds'].shape)\n",
    "print(\"Testing data shape: \", test_data.encodings['inputs_embeds'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789826a",
   "metadata": {},
   "source": [
    "### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"./data/pretrain\"\n",
    "\n",
    "if not os.path.exists(parent_dir):\n",
    "    os.makedirs(parent_dir)\n",
    "    torch.save(train_data, os.path.join(parent_dir, \"train_data.pt\"))\n",
    "    torch.save(test_data, os.path.join(parent_dir, \"test_data.pt\"))\n",
    "    print(\"Saved successfully\")\n",
    "elif os.path.exists(parent_dir):\n",
    "    check = input(\"Directory exist, overwrite? (y/n)\")\n",
    "    if check == 'y':\n",
    "        torch.save(train_data, os.path.join(parent_dir, \"train_data.pt\"))\n",
    "        torch.save(test_data, os.path.join(parent_dir, \"test_data.pt\"))\n",
    "        print(\"Saved successfully\")\n",
    "    else:\n",
    "        print(\"Saving failed\")\n",
    "else:\n",
    "    print(\"Saving failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1a478",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05baa7",
   "metadata": {},
   "source": [
    "Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding: arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "Harsuko, R., and T. Alkhalifah, 2022, Storseismic: An approach to pre-train a neural network to store seismic data features: 83rd EAGE Annual Conference & Exhibition, European Association of Geoscientists & Engineers, 1â€“5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storseismic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
